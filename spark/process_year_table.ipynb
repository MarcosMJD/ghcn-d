{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85f89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, IntegerType, DateType, StructField, StringType, TimestampType\n",
    "import logging, traceback\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d2d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Used when submitting job to spark master with parameters\n",
    "start_year = int(sys.argv[1])\n",
    "end_year = int(sys.argv[2])\n",
    "\"\"\"\n",
    "start_year = 2022\n",
    "end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ingestion to local (used when developing)\n",
    "URL_PREFIX = 'https://noaa-ghcn-pds.s3.amazonaws.com'\n",
    "TEMP_STORAGE_PATH = '/home/marcos/ghcn-d/spark/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c38f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/21 14:55:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/04/21 14:55:33 WARN DependencyUtils: Local jar /home/marcos/ghcn-d/spark/gcs-connector-hadoop3-latest.jar does not exist, skipping.\n",
      "22/04/21 14:55:33 WARN DependencyUtils: Local jar /home/marcos/ghcn-d/spark/spark-bigquery-with-dependencies_2.12-0.24.2.jar does not exist, skipping.\n",
      "22/04/21 14:55:33 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/04/21 14:55:33 INFO ResourceUtils: ==============================================================\n",
      "22/04/21 14:55:33 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/04/21 14:55:33 INFO ResourceUtils: ==============================================================\n",
      "22/04/21 14:55:33 INFO SparkContext: Submitted application: test\n",
      "22/04/21 14:55:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/04/21 14:55:34 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/04/21 14:55:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/04/21 14:55:34 INFO SecurityManager: Changing view acls to: marcos\n",
      "22/04/21 14:55:34 INFO SecurityManager: Changing modify acls to: marcos\n",
      "22/04/21 14:55:34 INFO SecurityManager: Changing view acls groups to: \n",
      "22/04/21 14:55:34 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/04/21 14:55:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marcos); groups with view permissions: Set(); users  with modify permissions: Set(marcos); groups with modify permissions: Set()\n",
      "22/04/21 14:55:34 INFO Utils: Successfully started service 'sparkDriver' on port 35437.\n",
      "22/04/21 14:55:34 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/04/21 14:55:34 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/04/21 14:55:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/04/21 14:55:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/04/21 14:55:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/21 14:55:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8ce73343-f0c5-4968-a358-1e31a25f2756\n",
      "22/04/21 14:55:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "22/04/21 14:55:34 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/21 14:55:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/04/21 14:55:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ghcnd.europe-west6-a.c.ghcn-d.internal:4040\n",
      "22/04/21 14:55:35 ERROR SparkContext: Failed to add gcs-connector-hadoop3-latest.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/marcos/ghcn-d/spark/gcs-connector-hadoop3-latest.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:55:35 ERROR SparkContext: Failed to add spark-bigquery-with-dependencies_2.12-0.24.2.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/marcos/ghcn-d/spark/spark-bigquery-with-dependencies_2.12-0.24.2.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:55:35 INFO Executor: Starting executor ID driver on host ghcnd.europe-west6-a.c.ghcn-d.internal\n",
      "22/04/21 14:55:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40217.\n",
      "22/04/21 14:55:35 INFO NettyBlockTransferService: Server created on ghcnd.europe-west6-a.c.ghcn-d.internal:40217\n",
      "22/04/21 14:55:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/04/21 14:55:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ghcnd.europe-west6-a.c.ghcn-d.internal, 40217, None)\n",
      "22/04/21 14:55:35 INFO BlockManagerMasterEndpoint: Registering block manager ghcnd.europe-west6-a.c.ghcn-d.internal:40217 with 434.4 MiB RAM, BlockManagerId(driver, ghcnd.europe-west6-a.c.ghcn-d.internal, 40217, None)\n",
      "22/04/21 14:55:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ghcnd.europe-west6-a.c.ghcn-d.internal, 40217, None)\n",
      "22/04/21 14:55:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ghcnd.europe-west6-a.c.ghcn-d.internal, 40217, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:55:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/04/21 14:55:35 INFO SharedState: Warehouse path is 'file:/home/marcos/ghcn-d/spark/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "# For local spark master\n",
    "conf = pyspark.SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.12-0.24.2.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/marcos/.google/credentials/google_credentials.json\") \\\n",
    "    .set(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the connector.\n",
    "bucket = \"ghcnd_raw\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7a9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only when developing with local spark master\n",
    "def download_file(url, local_file_path):\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4031bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year, mode, df_stations, df_countries):\n",
    "\n",
    "  \"\"\"\n",
    "  # For developing process read directly from origin\n",
    "  csv_file_name = f'/{year}.csv'\n",
    "  dataset_url = URL_PREFIX + '/csv' + csv_file_name\n",
    "  csv_file_path = TEMP_STORAGE_PATH + csv_file_name\n",
    "\n",
    "  download_file(dataset_url, csv_file_path)    \n",
    "\n",
    "  schema = StructType([\n",
    "      StructField(\"id\", StringType(), True),\n",
    "      StructField(\"date\", IntegerType(), True),\n",
    "      StructField(\"element\", StringType(), True),   \n",
    "      StructField(\"value\", IntegerType(), True),   \n",
    "      StructField(\"m_flag\", StringType(), True),   \n",
    "      StructField(\"q_flag\", StringType(), True),   \n",
    "      StructField(\"s_flag\", StringType(), True),\n",
    "      StructField(\"obs_time\",IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "  df = spark.read \\\n",
    "    .options(header=False)\n",
    "    .schema(schema)\n",
    "    .csv(csv_file_path)\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Option, read from BQ\n",
    "  df = spark.read.format('bigquery') \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','ghcnd') \\\n",
    "    .option('table',f'{year}').load()\n",
    "\n",
    "\n",
    "  # Option, read from GCS\n",
    "  #df = spark.read.parquet(f'gs://ghcnd_raw/{year}.parquet')\n",
    "\n",
    "  print(f'processing year {year}...')\n",
    "  # Only used when reading from csv in order to convert to date. \n",
    "  # If reading from BQ, this is already done\n",
    "  # df = df.withColumn(\"date\", F.to_date(df.date.cast(\"string\"), \"yyyyMMdd\"))\n",
    "\n",
    "  df = df \\\n",
    "    .drop(\"q_flag\") \\\n",
    "    .withColumn(\"tmax\", \n",
    "          F.when(df.element == \"TMAX\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"tmin\", \n",
    "          F.when(df.element == \"TMIN\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"prcp\", F.when(df.element == \"PRCP\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snow\", F.when(df.element == \"SNOW\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snwd\", F.when(df.element == \"SNWD\", df.value.cast(\"double\")).otherwise(None))\n",
    "\n",
    "  df_daily = df \\\n",
    "      .groupBy(\"id\", \"date\").agg( \n",
    "          F.avg(\"tmax\"),\n",
    "          F.avg(\"tmin\"),\n",
    "          F.avg(\"prcp\"),\n",
    "          F.avg(\"snow\"),\n",
    "          F.avg(\"snwd\"),\n",
    "          F.first(\"m_flag\"),\n",
    "          F.first(\"s_flag\")\n",
    "      ) \\\n",
    "      .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "      .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "      .drop ('station_id', 'code') \\\n",
    "      .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \n",
    "\n",
    "  # Note: toDF after joins, otherwise join will raise error\n",
    "  # Note: toDF since BQ does not allow field names with () and average generates these kind of names avg(tmax)\n",
    "\n",
    "  df_yearly =  df \\\n",
    "    .withColumn(\"date\", F.trunc(\"date\", \"year\")) \\\n",
    "    .groupBy(\"id\", \"date\").agg( \n",
    "      F.avg(\"tmax\"),\n",
    "      F.avg(\"tmin\"),\n",
    "      F.avg(\"prcp\"),\n",
    "      F.avg(\"snow\"),\n",
    "      F.avg(\"snwd\"),\n",
    "      F.first(\"m_flag\"),\n",
    "      F.first(\"s_flag\")\n",
    "    ) \\\n",
    "    .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "    .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "    .drop ('station_id', 'code') \\\n",
    "    .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \\\n",
    "\n",
    "  # For some reason, partition by date does not work after F.year(\"date\"). This has to be fixed\n",
    "  # Also, partition is needed for clustering\n",
    "  df_yearly.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"clusteredFields\", \"date, country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark_yearly') \\\n",
    "    .save()\n",
    "    \n",
    "  \n",
    "  df_daily.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"partitionField\", \"date\") \\\n",
    "    .option(\"partitionType\", \"YEAR\") \\\n",
    "    .option(\"clusteredFields\", \"country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark') \\\n",
    "    .save()\n",
    "  \n",
    "\n",
    "\n",
    "  print(f'process {year} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275b2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use if needed to read from BigQuery instead of GCS\n",
    "df_stations = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'stations').load() \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'countries').load() \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8305e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_stations = spark.read.parquet('gs://ghcnd_raw/ghcnd-stations.parquet')   .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id')   .withColumnRenamed('name', 'station_name')   .withColumnRenamed('id', 'station_id')   .withColumn('country_code', F.substring('station_id', 0, 2))\\n\\ndf_countries = spark.read.parquet('gs://ghcnd_raw/ghcnd-countries.parquet')   .withColumnRenamed('name', 'country_name')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df_stations = spark.read.parquet('gs://ghcnd_raw/ghcnd-stations.parquet') \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.parquet('gs://ghcnd_raw/ghcnd-countries.parquet') \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c589e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing year 2022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:55:48 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.2022, parameters sent from Spark: requiredColumns=[id,m_flag,s_flag,date,element,value], filters=[]\n",
      "22/04/21 14:55:48 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.2022 columns=[id, m_flag, s_flag, date, element, value], filter=''\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.2022': projects/ghcn-d/locations/europe-west6/sessions/CAISDE5ZMmFmR1F3eWoxehoCaHcaAmh4\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.stations, parameters sent from Spark: requiredColumns=[id,latitude,longitude,elevation,name], filters=[IsNotNull(id)]\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.stations columns=[id, latitude, longitude, elevation, name], filter='(`id` IS NOT NULL)'\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.stations': projects/ghcn-d/locations/europe-west6/sessions/CAISDExJbGR5c1lwQWlRURoCaHcaAmh4\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.countries, parameters sent from Spark: requiredColumns=[code,name], filters=[IsNotNull(code)]\n",
      "22/04/21 14:55:50 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.countries columns=[code, name], filter='(`code` IS NOT NULL)'\n",
      "22/04/21 14:55:51 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.countries': projects/ghcn-d/locations/europe-west6/sessions/CAISDGhWLTFKdC14bk1ycRoCaHcaAmh4\n",
      "22/04/21 14:55:51 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:55:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:55:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:55:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:55:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:55:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:55:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:55:52 INFO CodeGenerator: Code generated in 297.892607 ms\n",
      "22/04/21 14:55:52 INFO CodeGenerator: Code generated in 316.386252 ms\n",
      "22/04/21 14:55:52 INFO CodeGenerator: Code generated in 359.063099 ms\n",
      "22/04/21 14:55:52 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:55:52 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Got job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Final stage: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "22/04/21 14:55:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 17.7 KiB, free 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 8.0 KiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:55:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "22/04/21 14:55:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.7 KiB, free 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 7.4 KiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:55:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:55:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4396 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Registering RDD 9 (save at BigQueryWriteHelper.scala:64) as input to shuffle 0\n",
      "22/04/21 14:55:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4396 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Got map stage job 2 (save at BigQueryWriteHelper.scala:64) with 1 output partitions\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at BigQueryWriteHelper.scala:64)\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:55:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "22/04/21 14:55:52 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at save at BigQueryWriteHelper.scala:64), which has no missing parents\n",
      "22/04/21 14:55:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "22/04/21 14:55:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 45.9 KiB, free 434.3 MiB)\n",
      "22/04/21 14:55:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.3 MiB)\n",
      "22/04/21 14:55:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 19.6 KiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:55:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at save at BigQueryWriteHelper.scala:64) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:55:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:55:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4385 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:55:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "22/04/21 14:55:53 INFO CodeGenerator: Code generated in 94.716762 ms(0 + 1) / 1]\n",
      "22/04/21 14:55:53 INFO CodeGenerator: Code generated in 22.178496 ms\n",
      "22/04/21 14:55:53 INFO BaseAllocator: Debug mode disabled.          (0 + 1) / 1]\n",
      "22/04/21 14:55:53 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type\n",
      "22/04/21 14:55:53 INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:55:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 6158 bytes result sent to driver\n",
      "22/04/21 14:55:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1483 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:55:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:55:54 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.609 s\n",
      "22/04/21 14:55:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/04/21 14:55:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "22/04/21 14:55:54 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.966297 s\n",
      "22/04/21 14:55:54 INFO CodeGenerator: Code generated in 15.479927 ms            \n",
      "22/04/21 14:55:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.0 MiB, free 426.0 MiB)\n",
      "22/04/21 14:55:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 426.0 MiB)\n",
      "22/04/21 14:55:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 6.3 KiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:54 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:55:55 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 5.0 MiB, free 411.3 MiB)\n",
      "22/04/21 14:55:55 INFO BlockManagerInfo: Added taskresult_0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 5.0 MiB, free: 429.4 MiB)\n",
      "22/04/21 14:55:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5223228 bytes result sent via BlockManager)\n",
      "22/04/21 14:55:56 INFO TransportClientFactory: Successfully created connection to ghcnd.europe-west6-a.c.ghcn-d.internal/10.172.15.195:40217 after 188 ms (0 ms spent in bootstraps)\n",
      "22/04/21 14:55:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 7.4 KiB, free: 429.4 MiB)\n",
      "22/04/21 14:55:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3719 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:55:56 INFO DAGScheduler: ResultStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 4.000 s\n",
      "22/04/21 14:55:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/04/21 14:55:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:55:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "22/04/21 14:55:56 INFO DAGScheduler: Job 0 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 4.100468 s\n",
      "22/04/21 14:55:56 INFO BlockManagerInfo: Removed taskresult_0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 5.0 MiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 24.0 MiB, free 370.3 MiB)\n",
      "22/04/21 14:55:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 8.0 KiB, free: 434.4 MiB)\n",
      "22/04/21 14:55:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 342.3 MiB)\n",
      "22/04/21 14:55:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 4.0 MiB, free: 430.4 MiB)\n",
      "22/04/21 14:55:57 INFO MemoryStore: Block broadcast_4_piece1 stored as bytes in memory (estimated size 2000.7 KiB, free 340.4 MiB)\n",
      "22/04/21 14:55:57 INFO BlockManagerInfo: Added broadcast_4_piece1 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 2000.7 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:55:57 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:56:02 INFO UnsafeExternalSorter: Thread 91 spilling sort data of 400.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:56:13 INFO UnsafeExternalSorter: Thread 91 spilling sort data of 400.0 MiB to disk (1  time so far)\n",
      "22/04/21 14:56:22 INFO UnsafeExternalSorter: Thread 91 spilling sort data of 400.0 MiB to disk (2  times so far)\n",
      "22/04/21 14:56:29 INFO CodeGenerator: Code generated in 16.817 ms\n",
      "22/04/21 14:56:29 INFO CodeGenerator: Code generated in 66.387127 ms\n",
      "22/04/21 14:56:29 INFO CodeGenerator: Code generated in 17.528216 ms\n",
      "22/04/21 14:56:29 INFO CodeGenerator: Code generated in 23.243705 ms\n",
      "22/04/21 14:56:29 INFO CodeGenerator: Code generated in 15.998041 ms\n",
      "22/04/21 14:57:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2401 bytes result sent to driver\n",
      "22/04/21 14:57:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 89668 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:57:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:57:22 INFO DAGScheduler: ShuffleMapStage 2 (save at BigQueryWriteHelper.scala:64) finished in 89.799 s\n",
      "22/04/21 14:57:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/04/21 14:57:22 INFO DAGScheduler: running: Set()\n",
      "22/04/21 14:57:22 INFO DAGScheduler: waiting: Set()\n",
      "22/04/21 14:57:22 INFO DAGScheduler: failed: Set()\n",
      "22/04/21 14:57:22 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/04/21 14:57:22 INFO CodeGenerator: Code generated in 29.206673 ms\n",
      "22/04/21 14:57:22 INFO CodeGenerator: Code generated in 16.394416 ms\n",
      "22/04/21 14:57:23 INFO SparkContext: Starting job: save at BigQueryWriteHelper.scala:64\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Got job 3 (save at BigQueryWriteHelper.scala:64) with 1 output partitions\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Final stage: ResultStage 4 (save at BigQueryWriteHelper.scala:64)\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at save at BigQueryWriteHelper.scala:64), which has no missing parents\n",
      "22/04/21 14:57:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 257.0 KiB, free 400.1 MiB)\n",
      "22/04/21 14:57:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 92.9 KiB, free 400.0 MiB)\n",
      "22/04/21 14:57:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 92.9 KiB, free: 428.3 MiB)\n",
      "22/04/21 14:57:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:57:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at save at BigQueryWriteHelper.scala:64) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:57:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:57:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:57:23 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "22/04/21 14:57:23 INFO ShuffleBlockFetcherIterator: Getting 1 (1348.5 KiB) non-empty blocks including 1 (1348.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/04/21 14:57:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms\n",
      "22/04/21 14:57:23 INFO CodeGenerator: Code generated in 11.792626 ms\n",
      "22/04/21 14:57:23 INFO CodeGenerator: Code generated in 31.812517 ms\n",
      "22/04/21 14:57:23 INFO CodeGenerator: Code generated in 9.194606 ms\n",
      "22/04/21 14:57:23 INFO CodeGenerator: Code generated in 18.670838 ms\n",
      "22/04/21 14:57:23 INFO CodeGenerator: Code generated in 11.215267 ms\n",
      "22/04/21 14:57:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:57:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:57:23 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:57:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:57:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:57:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:57:23 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:57:23 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:57:23 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "22/04/21 14:57:23 INFO ParquetOutputFormat: Validation is off\n",
      "22/04/21 14:57:23 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "22/04/21 14:57:23 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "22/04/21 14:57:23 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmax\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmin\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"prcp\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snow\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snwd\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"m_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"s_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"latitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"longitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"elevation\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"station_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double tmax;\n",
      "  optional double tmin;\n",
      "  optional double prcp;\n",
      "  optional double snow;\n",
      "  optional double snwd;\n",
      "  optional binary m_flag (STRING);\n",
      "  optional binary s_flag (STRING);\n",
      "  optional double latitude;\n",
      "  optional double longitude;\n",
      "  optional double elevation;\n",
      "  optional binary station_name (STRING);\n",
      "  optional binary country_code (STRING);\n",
      "  optional binary country_name (STRING);\n",
      "}\n",
      "\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:57:23 INFO CodecPool: Got brand-new compressor [.snappy](0 + 1) / 1]\n",
      "22/04/21 14:57:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://ghcnd_raw/.spark-bigquery-local-1650552935200-01ca10a0-9961-46df-825e-97a7421cac08/_temporary/0/_temporary/' directory.\n",
      "22/04/21 14:57:26 INFO FileOutputCommitter: Saved output of task 'attempt_20220421145722171570773572826873_0004_m_000000_3' to gs://ghcnd_raw/.spark-bigquery-local-1650552935200-01ca10a0-9961-46df-825e-97a7421cac08/_temporary/0/task_20220421145722171570773572826873_0004_m_000000\n",
      "22/04/21 14:57:26 INFO SparkHadoopMapRedUtil: attempt_20220421145722171570773572826873_0004_m_000000_3: Committed\n",
      "22/04/21 14:57:26 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 4525 bytes result sent to driver\n",
      "22/04/21 14:57:26 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 3038 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:57:26 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:57:26 INFO DAGScheduler: ResultStage 4 (save at BigQueryWriteHelper.scala:64) finished in 3.098 s\n",
      "22/04/21 14:57:26 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/04/21 14:57:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "22/04/21 14:57:26 INFO DAGScheduler: Job 3 finished: save at BigQueryWriteHelper.scala:64, took 3.140259 s\n",
      "22/04/21 14:57:26 INFO FileFormatWriter: Start to commit write Job 4c2fef3c-c74c-4069-89f1-ff452e5bd18c.\n",
      "22/04/21 14:57:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://ghcnd_raw/.spark-bigquery-local-1650552935200-01ca10a0-9961-46df-825e-97a7421cac08/_temporary/0/task_20220421145722171570773572826873_0004_m_000000/' directory.\n",
      "22/04/21 14:57:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://ghcnd_raw/.spark-bigquery-local-1650552935200-01ca10a0-9961-46df-825e-97a7421cac08/' directory.\n",
      "22/04/21 14:57:26 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 92.9 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:26 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 19.6 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:26 INFO FileFormatWriter: Write Job 4c2fef3c-c74c-4069-89f1-ff452e5bd18c committed. Elapsed time: 627 ms.\n",
      "22/04/21 14:57:26 INFO FileFormatWriter: Finished processing stats for write job 4c2fef3c-c74c-4069-89f1-ff452e5bd18c.\n",
      "22/04/21 14:57:27 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=production, projectId=ghcn-d, tableId=fact_observations_spark_yearly}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=null, ignoreUnknownValue=null, sourceUris=[gs://ghcnd_raw/.spark-bigquery-local-1650552935200-01ca10a0-9961-46df-825e-97a7421cac08/part-00000-86fd6e47-3557-453e-8f82-50464cdb0db7-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=true, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null}. jobId: JobId{project=ghcn-d, job=34f555f5-7df9-4b49-aed5-1943af6fd529, location=europe-west6}\n",
      "22/04/21 14:57:30 INFO BigQueryClient: Done loading to ghcn-d.production.fact_observations_spark_yearly. jobId: JobId{project=ghcn-d, job=34f555f5-7df9-4b49-aed5-1943af6fd529, location=europe-west6}\n",
      "22/04/21 14:57:31 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.2022, parameters sent from Spark: requiredColumns=[id,m_flag,s_flag,date,element,value], filters=[]\n",
      "22/04/21 14:57:31 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.2022 columns=[id, m_flag, s_flag, date, element, value], filter=''\n",
      "22/04/21 14:57:31 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.2022': projects/ghcn-d/locations/europe-west6/sessions/CAISDDhzLV9FVWtqVUl4axoCaHcaAmh4\n",
      "22/04/21 14:57:31 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.stations, parameters sent from Spark: requiredColumns=[id,latitude,longitude,elevation,name], filters=[IsNotNull(id)]\n",
      "22/04/21 14:57:31 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.stations columns=[id, latitude, longitude, elevation, name], filter='(`id` IS NOT NULL)'\n",
      "22/04/21 14:57:32 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.stations': projects/ghcn-d/locations/europe-west6/sessions/CAISDEhuV203VmVoNENGVhoCaHcaAmh4\n",
      "22/04/21 14:57:32 INFO DirectBigQueryRelation: Querying table ghcn-d.ghcnd.countries, parameters sent from Spark: requiredColumns=[code,name], filters=[IsNotNull(code)]\n",
      "22/04/21 14:57:32 INFO DirectBigQueryRelation: Going to read from ghcn-d.ghcnd.countries columns=[code, name], filter='(`code` IS NOT NULL)'\n",
      "22/04/21 14:57:32 INFO DirectBigQueryRelation: Created read session for table 'ghcn-d.ghcnd.countries': projects/ghcn-d/locations/europe-west6/sessions/CAISDE8zaFVQdW9XQ3NVQhoCaHcaAmh4\n",
      "22/04/21 14:57:32 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:57:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:57:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:57:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:57:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:57:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:57:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:57:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Final stage: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 17.7 KiB, free 400.4 MiB)\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 400.4 MiB)\n",
      "22/04/21 14:57:32 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 8.0 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:32 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:57:32 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:57:32 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4396 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:57:32 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
      "22/04/21 14:57:32 INFO CodeGenerator: Code generated in 41.393855 ms\n",
      "22/04/21 14:57:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.7 KiB, free 400.4 MiB)\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 400.4 MiB)\n",
      "22/04/21 14:57:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 7.4 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:32 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:57:32 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:57:32 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4396 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Registering RDD 23 (save at BigQueryWriteHelper.scala:64) as input to shuffle 1\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Got map stage job 6 (save at BigQueryWriteHelper.scala:64) with 1 output partitions\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (save at BigQueryWriteHelper.scala:64)\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[23] at save at BigQueryWriteHelper.scala:64), which has no missing parents\n",
      "22/04/21 14:57:32 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 45.4 KiB, free 400.3 MiB)\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.3 KiB, free 400.3 MiB)\n",
      "22/04/21 14:57:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 19.3 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:32 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[23] at save at BigQueryWriteHelper.scala:64) (first 15 tasks are for partitions Vector(0))\n",
      "22/04/21 14:57:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "22/04/21 14:57:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, PROCESS_LOCAL, 4385 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:57:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:57:32 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 6072 bytes result sent to driver\n",
      "22/04/21 14:57:32 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 257 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:57:32 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:57:32 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.270 s\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/04/21 14:57:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "22/04/21 14:57:32 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.277867 s\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.0 MiB, free 396.3 MiB)\n",
      "22/04/21 14:57:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 396.2 MiB)\n",
      "22/04/21 14:57:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 6.3 KiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:32 INFO SparkContext: Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:57:33 INFO MemoryStore: Block taskresult_4 stored as bytes in memory (estimated size 5.0 MiB, free 381.3 MiB)\n",
      "22/04/21 14:57:33 INFO BlockManagerInfo: Added taskresult_4 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 5.0 MiB, free: 423.4 MiB)\n",
      "22/04/21 14:57:33 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 5223185 bytes result sent via BlockManager)\n",
      "22/04/21 14:57:33 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 7.4 KiB, free: 423.4 MiB)\n",
      "22/04/21 14:57:33 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 675 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:57:33 INFO DAGScheduler: ResultStage 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.693 s\n",
      "22/04/21 14:57:33 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/04/21 14:57:33 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:57:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "22/04/21 14:57:33 INFO BlockManagerInfo: Removed taskresult_4 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 5.0 MiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:33 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.704559 s\n",
      "22/04/21 14:57:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 24.0 MiB, free 352.3 MiB)\n",
      "22/04/21 14:57:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 344.3 MiB)\n",
      "22/04/21 14:57:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 4.0 MiB, free: 424.4 MiB)\n",
      "22/04/21 14:57:33 INFO MemoryStore: Block broadcast_10_piece1 stored as bytes in memory (estimated size 2000.7 KiB, free 342.4 MiB)\n",
      "22/04/21 14:57:33 INFO BlockManagerInfo: Added broadcast_10_piece1 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 2000.7 KiB, free: 422.5 MiB)\n",
      "22/04/21 14:57:33 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "22/04/21 14:57:34 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 8.0 KiB, free: 422.5 MiB)\n",
      "22/04/21 14:57:36 INFO UnsafeExternalSorter: Thread 198 spilling sort data of 364.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:57:43 INFO UnsafeExternalSorter: Thread 198 spilling sort data of 364.0 MiB to disk (1  time so far)\n",
      "22/04/21 14:57:43 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 6.3 KiB, free: 422.5 MiB)\n",
      "22/04/21 14:57:43 INFO BlockManagerInfo: Removed broadcast_4_piece1 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 2000.7 KiB, free: 424.4 MiB)\n",
      "22/04/21 14:57:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 4.0 MiB, free: 428.4 MiB)\n",
      "22/04/21 14:57:50 INFO UnsafeExternalSorter: Thread 198 spilling sort data of 400.0 MiB to disk (2  times so far)\n",
      "22/04/21 14:58:58 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 2358 bytes result sent to driver\n",
      "22/04/21 14:58:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 86355 ms on ghcnd.europe-west6-a.c.ghcn-d.internal (executor driver) (1/1)\n",
      "22/04/21 14:58:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "22/04/21 14:58:58 INFO DAGScheduler: ShuffleMapStage 7 (save at BigQueryWriteHelper.scala:64) finished in 86.412 s\n",
      "22/04/21 14:58:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/04/21 14:58:58 INFO DAGScheduler: running: Set()\n",
      "22/04/21 14:58:58 INFO DAGScheduler: waiting: Set()\n",
      "22/04/21 14:58:58 INFO DAGScheduler: failed: Set()\n",
      "22/04/21 14:58:58 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 19363400, minimum partition size: 1048576\n",
      "22/04/21 14:58:59 INFO SparkContext: Starting job: save at BigQueryWriteHelper.scala:64\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Got job 7 (save at BigQueryWriteHelper.scala:64) with 4 output partitions\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Final stage: ResultStage 9 (save at BigQueryWriteHelper.scala:64)\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Missing parents: List()\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[27] at save at BigQueryWriteHelper.scala:64), which has no missing parents\n",
      "22/04/21 14:58:59 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 256.6 KiB, free 400.1 MiB)\n",
      "22/04/21 14:58:59 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 92.9 KiB, free 400.0 MiB)\n",
      "22/04/21 14:58:59 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 (size: 92.9 KiB, free: 428.3 MiB)\n",
      "22/04/21 14:58:59 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1478\n",
      "22/04/21 14:58:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at save at BigQueryWriteHelper.scala:64) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "22/04/21 14:58:59 INFO TaskSchedulerImpl: Adding task set 9.0 with 4 tasks resource profile 0\n",
      "22/04/21 14:58:59 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:58:59 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 8) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:58:59 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 9) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 2, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:58:59 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 10) (ghcnd.europe-west6-a.c.ghcn-d.internal, executor driver, partition 3, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/04/21 14:58:59 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)\n",
      "22/04/21 14:58:59 INFO Executor: Running task 2.0 in stage 9.0 (TID 9)\n",
      "22/04/21 14:58:59 INFO Executor: Running task 3.0 in stage 9.0 (TID 10)\n",
      "22/04/21 14:58:59 INFO Executor: Running task 1.0 in stage 9.0 (TID 8)\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Getting 1 (18.5 MiB) non-empty blocks including 1 (18.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Getting 1 (18.5 MiB) non-empty blocks including 1 (18.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Getting 1 (18.5 MiB) non-empty blocks including 1 (18.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Getting 1 (18.5 MiB) non-empty blocks including 1 (18.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/04/21 14:58:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:58:59 INFO UnsafeExternalSorter: Thread 198 spilling sort data of 100.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:58:59 INFO UnsafeExternalSorter: Thread 240 spilling sort data of 100.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:58:59 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ghcnd.europe-west6-a.c.ghcn-d.internal:40217 in memory (size: 19.3 KiB, free: 428.3 MiB)\n",
      "22/04/21 14:58:59 INFO UnsafeExternalSorter: Thread 238 spilling sort data of 100.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:58:59 INFO UnsafeExternalSorter: Thread 239 spilling sort data of 100.0 MiB to disk (0  time so far)\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Validation is off\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Validation is off\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "22/04/21 14:59:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmax\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmin\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"prcp\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snow\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snwd\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"m_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"s_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"latitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"longitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"elevation\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"station_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double tmax;\n",
      "  optional double tmin;\n",
      "  optional double prcp;\n",
      "  optional double snow;\n",
      "  optional double snwd;\n",
      "  optional binary m_flag (STRING);\n",
      "  optional binary s_flag (STRING);\n",
      "  optional double latitude;\n",
      "  optional double longitude;\n",
      "  optional double elevation;\n",
      "  optional binary station_name (STRING);\n",
      "  optional binary country_code (STRING);\n",
      "  optional binary country_name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "22/04/21 14:59:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmax\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmin\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"prcp\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snow\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snwd\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"m_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"s_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"latitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"longitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"elevation\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"station_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double tmax;\n",
      "  optional double tmin;\n",
      "  optional double prcp;\n",
      "  optional double snow;\n",
      "  optional double snwd;\n",
      "  optional binary m_flag (STRING);\n",
      "  optional binary s_flag (STRING);\n",
      "  optional double latitude;\n",
      "  optional double longitude;\n",
      "  optional double elevation;\n",
      "  optional binary station_name (STRING);\n",
      "  optional binary country_code (STRING);\n",
      "  optional binary country_name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Validation is off\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:59:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmax\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmin\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"prcp\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snow\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snwd\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"m_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"s_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"latitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"longitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"elevation\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"station_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double tmax;\n",
      "  optional double tmin;\n",
      "  optional double prcp;\n",
      "  optional double snow;\n",
      "  optional double snwd;\n",
      "  optional binary m_flag (STRING);\n",
      "  optional binary s_flag (STRING);\n",
      "  optional double latitude;\n",
      "  optional double longitude;\n",
      "  optional double elevation;\n",
      "  optional binary station_name (STRING);\n",
      "  optional binary country_code (STRING);\n",
      "  optional binary country_name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/04/21 14:59:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/04/21 14:59:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO CodecConfig: Compression: SNAPPY\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Validation is off\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "22/04/21 14:59:01 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "22/04/21 14:59:01 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmax\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"tmin\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"prcp\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snow\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"snwd\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"m_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"s_flag\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"latitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"longitude\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"elevation\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"station_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_code\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"country_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double tmax;\n",
      "  optional double tmin;\n",
      "  optional double prcp;\n",
      "  optional double snow;\n",
      "  optional double snwd;\n",
      "  optional binary m_flag (STRING);\n",
      "  optional binary s_flag (STRING);\n",
      "  optional double latitude;\n",
      "  optional double longitude;\n",
      "  optional double elevation;\n",
      "  optional binary station_name (STRING);\n",
      "  optional binary country_code (STRING);\n",
      "  optional binary country_name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "22/04/21 14:59:02 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "22/04/21 14:59:02 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "22/04/21 14:59:02 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "22/04/21 14:59:12 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:59:12 WARN FileOutputCommitter: Could not delete gs://ghcnd_raw/.spark-bigquery-local-1650552935200-866f1e42-f03d-482d-a613-9d072343b3a7/_temporary/0/_temporary/attempt_202204211458591456269191956925636_0009_m_000000_7\n",
      "22/04/21 14:59:12 ERROR FileFormatWriter: Job job_202204211458591456269191956925636_0009 aborted.\n",
      "22/04/21 14:59:12 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 7)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "22/04/21 14:59:12 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 9.0 (TID 7),5,main]\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "22/04/21 14:59:12 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 7) (ghcnd.europe-west6-a.c.ghcn-d.internal executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "\n",
      "22/04/21 14:59:12 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "22/04/21 14:59:12 INFO TaskSchedulerImpl: Cancelling stage 9\n",
      "22/04/21 14:59:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage cancelled\n",
      "22/04/21 14:59:12 INFO Executor: Executor is trying to kill task 2.0 in stage 9.0 (TID 9), reason: Stage cancelled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:59:12 INFO Executor: Executor is trying to kill task 3.0 in stage 9.0 (TID 10), reason: Stage cancelled\n",
      "22/04/21 14:59:12 INFO Executor: Executor is trying to kill task 1.0 in stage 9.0 (TID 8), reason: Stage cancelled\n",
      "22/04/21 14:59:12 INFO TaskSchedulerImpl: Stage 9 was cancelled\n",
      "22/04/21 14:59:12 INFO DAGScheduler: ResultStage 9 (save at BigQueryWriteHelper.scala:64) failed in 13.870 s due to Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 7) (ghcnd.europe-west6-a.c.ghcn-d.internal executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "22/04/21 14:59:12 INFO DAGScheduler: Job 7 failed: save at BigQueryWriteHelper.scala:64, took 13.889959 s\n",
      "22/04/21 14:59:12 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:118)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:59:12 ERROR FileFormatWriter: Aborting job 557e3d4c-2b57-453c-b041-c3ac0b96592a.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 7) (ghcnd.europe-west6-a.c.ghcn-d.internal executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:64)\n",
      "\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n",
      "\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:112)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:321)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "22/04/21 14:59:12 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:118)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:59:12 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:309)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$SpillableIterator.loadNext(UnsafeExternalSorter.java:658)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.processCurrentSortedGroup(SortBasedAggregationIterator.scala:118)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:149)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.next(SortBasedAggregationIterator.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:59:12 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "\r",
      "[Stage 9:>                                                          (0 + 3) / 4]\r",
      "22/04/21 14:59:13 INFO SparkUI: Stopped Spark web UI at http://ghcnd.europe-west6-a.c.ghcn-d.internal:4040\n",
      "22/04/21 14:59:13 WARN FileOutputCommitter: Could not delete gs://ghcnd_raw/.spark-bigquery-local-1650552935200-866f1e42-f03d-482d-a613-9d072343b3a7/_temporary/0/_temporary/attempt_202204211458596837195460306519469_0009_m_000003_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/21 14:59:13 WARN Utils: Suppressing exception in catch: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.buildContentChunk(MediaHttpUploader.java:579)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:380)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:308)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:528)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565)\n",
      "\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:85)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/21 14:59:13 INFO Executor: Executor interrupted and killed task 3.0 in stage 9.0 (TID 10), reason: Stage cancelled\n",
      "22/04/21 14:59:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/04/21 14:59:13 INFO MemoryStore: MemoryStore cleared\n",
      "22/04/21 14:59:13 INFO BlockManager: BlockManager stopped\n",
      "22/04/21 14:59:13 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/04/21 14:59:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/04/21 14:59:13 INFO SparkContext: Successfully stopped SparkContext\n",
      "22/04/21 14:59:13 INFO ShutdownHookManager: Shutdown hook called\n",
      "22/04/21 14:59:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-21c16339-5120-4818-bd89-59d4e2048ad7\n",
      "22/04/21 14:59:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-21c16339-5120-4818-bd89-59d4e2048ad7/pyspark-94212171-14f3-47bb-97a6-26d971abfa68\n",
      "22/04/21 14:59:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-1ecf3f02-7c7f-4a1d-98c2-924b120baa45\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marcos/bin/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1146645/1108152240.py\", line 3, in <module>\n",
      "    process_year(year, 'overwrite', df_stations, df_countries)\n",
      "  File \"/tmp/ipykernel_1146645/299924469.py\", line 112, in process_year\n",
      "    df_daily.write \\\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\", line 738, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marcos/bin/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JJavaError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1146645/1108152240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprocess_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_countries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1146645/299924469.py\u001b[0m in \u001b[0;36mprocess_year\u001b[0;34m(year, mode, df_stations, df_countries)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0mdf_daily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_pdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m                         \u001b[0;31m# drop into debugger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/lib/python3.9/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36m_showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;34m'traceback'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;34m'ename'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;34m'evalue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         }\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mgateway_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_return_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# Note: technically this should return a bytestring 'str' rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    401\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 480, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "for year in range(start_year, end_year+1):\n",
    "  if year == start_year:\n",
    "    process_year(year, 'overwrite', df_stations, df_countries)\n",
    "  else:\n",
    "    process_year(year, 'append', df_stations, df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
