{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, IntegerType, DateType, StructField, StringType, TimestampType\n",
    "import logging, traceback\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PREFIX = 'https://noaa-ghcn-pds.s3.amazonaws.com'\n",
    "TEMP_STORAGE_PATH = '/home/marcos/ghcn-d/spark/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe94ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, local_file_path):\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "csv_file_name = f'/{year}.csv'\n",
    "dataset_url = URL_PREFIX + '/csv' + csv_file_name\n",
    "csv_file_path = TEMP_STORAGE_PATH + csv_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(dataset_url, csv_file_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", IntegerType(), True),\n",
    "    StructField(\"element\", StringType(), True),   \n",
    "    StructField(\"value\", IntegerType(), True),   \n",
    "    StructField(\"m_flag\", StringType(), True),   \n",
    "    StructField(\"q_flag\", StringType(), True),   \n",
    "    StructField(\"s_flag\", StringType(), True),\n",
    "    StructField(\"obs_time\",IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .options(header=False) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"date\", F.to_date(df.date.cast(\"string\"), \"yyyyMMdd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .drop(\"q_flag\") \\\n",
    "    .withColumn(\"tmax\", \n",
    "        F.when(df.element == \"TMAX\", \n",
    "            F.when(df.value > 700, None).otherwise(\n",
    "                F.when(df.value < -700, None). otherwise(\n",
    "                    df.value.cast(\"double\")/10)\n",
    "                )\n",
    "        ).otherwise(\"None\")\n",
    "     ) \\\n",
    "    .withColumn(\"tmin\", \n",
    "        F.when(df.element == \"TMIN\", \n",
    "            F.when(df.value > 700, None).otherwise(\n",
    "                F.when(df.value < -700, None). otherwise(\n",
    "                    df.value.cast(\"double\")/10)\n",
    "                )\n",
    "        ).otherwise(\"None\")\n",
    "     ) \\\n",
    "    .withColumn(\"prcp\", F.when(df.element == \"PRCP\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "    .withColumn(\"snow\", F.when(df.element == \"SNOW\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "    .withColumn(\"snwd\", F.when(df.element == \"SNWD\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "    .groupBy(\"id\", \"date\").agg( \n",
    "        F.avg(\"tmax\"),\n",
    "        F.avg(\"tmin\"),\n",
    "        F.avg(\"prcp\"),\n",
    "        F.avg(\"snow\"),\n",
    "        F.avg(\"snwd\"),\n",
    "        F.first(\"m_flag\"),\n",
    "        F.first(\"s_flag\")\n",
    "    ).show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320ff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3879ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
