{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85f89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, IntegerType, DateType, StructField, StringType, TimestampType\n",
    "import logging, traceback\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d2d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Used when submitting job to spark master with parameters\n",
    "start_year = int(sys.argv[1])\n",
    "end_year = int(sys.argv[2])\n",
    "\"\"\"\n",
    "start_year = 2022\n",
    "end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ingestion to local (used when developing)\n",
    "URL_PREFIX = 'https://noaa-ghcn-pds.s3.amazonaws.com'\n",
    "TEMP_STORAGE_PATH = '/home/marcos/ghcn-d/spark/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32fe94ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/marcos/.ivy2/cache\n",
      "The jars for the packages stored in: /home/marcos/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      "com.google.cloud.bigdataoss#gcs-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bd66f191-3b8e-4c44-a828-b298b464faa6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.24.2 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.0 in central\n",
      "\tfound com.google.api-client#google-api-client-jackson2;1.31.1 in central\n",
      "\tfound com.google.api-client#google-api-client;1.31.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.31.2 in central\n",
      "\tfound com.google.http-client#google-http-client;1.38.0 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.guava#guava;30.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.3.4 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound io.opencensus#opencensus-api;0.24.0 in central\n",
      "\tfound io.grpc#grpc-context;1.34.1 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.24.0 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.38.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#util;2.2.0 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.38.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.11.3 in central\n",
      "\tfound com.google.apis#google-api-services-iamcredentials;v1-rev20201022-1.31.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20201112-1.31.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.7.4 in central\n",
      "\tfound com.google.flogger#google-extensions;0.5.1 in central\n",
      "\tfound com.google.flogger#flogger;0.5.1 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.3 in central\n",
      "\tfound com.google.flogger#flogger-system-backend;0.5.1 in central\n",
      "\tfound com.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.0 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcsio;2.2.0 in central\n",
      "\tfound io.grpc#grpc-api;1.34.1 in central\n",
      "\tfound io.grpc#grpc-alts;1.34.1 in central\n",
      "\tfound io.grpc#grpc-auth;1.34.1 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;0.22.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.34.1 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.34.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.14.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;1.17.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.34.1 in central\n",
      "\tfound io.grpc#grpc-stub;1.34.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.5 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.1 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;0.22.0 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.34.1 in central\n",
      "\tfound io.grpc#grpc-core;1.34.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.0.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.14.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.6 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.18 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound io.perfmark#perfmark-api;0.19.0 in central\n",
      ":: resolution report :: resolve 1769ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.11.3 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;1.31.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client-jackson2;1.31.1 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.0.5 from central in [default]\n",
      "\tcom.google.apis#google-api-services-iamcredentials;v1-rev20201022-1.31.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20201112-1.31.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;0.22.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;0.22.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.7.4 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.0 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcsio;2.2.0 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util;2.2.0 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#util-hadoop;hadoop3-2.2.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.24.2 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.6 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.3.4 from central in [default]\n",
      "\tcom.google.flogger#flogger;0.5.1 from central in [default]\n",
      "\tcom.google.flogger#flogger-system-backend;0.5.1 from central in [default]\n",
      "\tcom.google.flogger#google-extensions;0.5.1 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;30.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.38.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.38.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.38.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.31.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.14.0 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.14.0 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-api;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-context;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-core;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.34.1 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.34.1 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.24.0 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.24.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.19.0 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.5 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.3 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.18 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.api.grpc#proto-google-common-protos;1.17.0 by [com.google.api.grpc#proto-google-common-protos;2.0.1] in [default]\n",
      "\tcom.google.http-client#google-http-client;1.37.0 by [com.google.http-client#google-http-client;1.38.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   56  |   0   |   0   |   2   ||   54  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bd66f191-3b8e-4c44-a828-b298b464faa6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 54 already retrieved (0kB/33ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 21:38:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# For local spark master\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.2,com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the connector.\n",
    "bucket = \"ghcnd_raw\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ddbe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set('fs.gs.impl', 'com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem')\n",
    "# This is required if you are using service account and set true, \n",
    "spark._jsc.hadoopConfiguration().set('fs.gs.auth.service.account.enable', 'true')\n",
    "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\",\"/home/marcos/.google/credentials/google_credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1e4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only when developing with local spark master\n",
    "def download_file(url, local_file_path):\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4031bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year, mode, df_stations, df_countries):\n",
    "\n",
    "  \"\"\"\n",
    "  # For developing process read directly from origin\n",
    "  csv_file_name = f'/{year}.csv'\n",
    "  dataset_url = URL_PREFIX + '/csv' + csv_file_name\n",
    "  csv_file_path = TEMP_STORAGE_PATH + csv_file_name\n",
    "\n",
    "  download_file(dataset_url, csv_file_path)    \n",
    "\n",
    "  schema = StructType([\n",
    "      StructField(\"id\", StringType(), True),\n",
    "      StructField(\"date\", IntegerType(), True),\n",
    "      StructField(\"element\", StringType(), True),   \n",
    "      StructField(\"value\", IntegerType(), True),   \n",
    "      StructField(\"m_flag\", StringType(), True),   \n",
    "      StructField(\"q_flag\", StringType(), True),   \n",
    "      StructField(\"s_flag\", StringType(), True),\n",
    "      StructField(\"obs_time\",IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "  df = spark.read \\\n",
    "    .options(header=False)\n",
    "    .schema(schema)\n",
    "    .csv(csv_file_path)\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Option, read from BQ\n",
    "  df = spark.read.format('bigquery') \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','ghcnd') \\\n",
    "    .option('table',f'{year}').load()\n",
    "\n",
    "\n",
    "  # Option, read from GCS\n",
    "  #df = spark.read.parquet(f'gs://ghcnd_raw/{year}.parquet')\n",
    "\n",
    "  print(f'processing year {year}...')\n",
    "  # Only used when reading from csv in order to convert to date. \n",
    "  # If reading from BQ, this is already done\n",
    "  # df = df.withColumn(\"date\", F.to_date(df.date.cast(\"string\"), \"yyyyMMdd\"))\n",
    "\n",
    "  df = df \\\n",
    "    .drop(\"q_flag\") \\\n",
    "    .withColumn(\"tmax\", \n",
    "          F.when(df.element == \"TMAX\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"tmin\", \n",
    "          F.when(df.element == \"TMIN\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"prcp\", F.when(df.element == \"PRCP\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snow\", F.when(df.element == \"SNOW\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snwd\", F.when(df.element == \"SNWD\", df.value.cast(\"double\")).otherwise(None))\n",
    "\n",
    "  df_daily = df \\\n",
    "      .groupBy(\"id\", \"date\").agg( \n",
    "          F.avg(\"tmax\"),\n",
    "          F.avg(\"tmin\"),\n",
    "          F.avg(\"prcp\"),\n",
    "          F.avg(\"snow\"),\n",
    "          F.avg(\"snwd\"),\n",
    "          F.first(\"m_flag\"),\n",
    "          F.first(\"s_flag\")\n",
    "      ) \\\n",
    "      .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "      .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "      .drop ('station_id', 'code') \\\n",
    "      .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \n",
    "\n",
    "  # Note: toDF after joins, otherwise join will raise error\n",
    "  # Note: toDF since BQ does not allow field names with () and average generates these kind of names avg(tmax)\n",
    "\n",
    "  df_yearly =  df \\\n",
    "    .withColumn(\"date\", F.trunc(\"date\", \"year\")) \\\n",
    "    .groupBy(\"id\", \"date\").agg( \n",
    "      F.avg(\"tmax\"),\n",
    "      F.avg(\"tmin\"),\n",
    "      F.avg(\"prcp\"),\n",
    "      F.avg(\"snow\"),\n",
    "      F.avg(\"snwd\"),\n",
    "      F.first(\"m_flag\"),\n",
    "      F.first(\"s_flag\")\n",
    "    ) \\\n",
    "    .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "    .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "    .drop ('station_id', 'code') \\\n",
    "    .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \\\n",
    "\n",
    "  # For some reason, partition by date does not work after F.year(\"date\"). This has to be fixed\n",
    "  # Also, partition is needed for clustering\n",
    "  df_yearly.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"clusteredFields\", \"date, country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark_yearly') \\\n",
    "    .save()\n",
    "    \n",
    "  \n",
    "  df_daily.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"partitionField\", \"date\") \\\n",
    "    .option(\"partitionType\", \"YEAR\") \\\n",
    "    .option(\"clusteredFields\", \"country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark') \\\n",
    "    .save()\n",
    "  \n",
    "\n",
    "\n",
    "  print(f'process {year} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "275b2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use if needed to read from BigQuery instead of GCS\n",
    "df_stations = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'stations').load() \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'countries').load() \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295f683a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|        country_name|\n",
      "+----+--------------------+\n",
      "|  FK|Falkland Islands ...|\n",
      "|  JO|              Jordan|\n",
      "|  PK|            Pakistan|\n",
      "|  RQ|Puerto Rico [Unit...|\n",
      "|  UK|      United Kingdom|\n",
      "|  BK|Bosnia and Herzeg...|\n",
      "|  LA|                Laos|\n",
      "|  LQ|Palmyra Atoll [Un...|\n",
      "|  PL|              Poland|\n",
      "|  ST|         Saint Lucia|\n",
      "|  SX|South Georgia and...|\n",
      "|  VM|             Vietnam|\n",
      "|  GV|              Guinea|\n",
      "|  TE|Tromelin Island [...|\n",
      "|  EK|   Equatorial Guinea|\n",
      "|  GA|         Gambia, The|\n",
      "|  IO|British Indian Oc...|\n",
      "|  MF|    Mayotte [France]|\n",
      "|  RP|         Philippines|\n",
      "|  TS|             Tunisia|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_countries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8305e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df_stations = spark.read.parquet('gs://ghcnd_raw/ghcnd-stations.parquet')   .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id')   .withColumnRenamed('name', 'station_name')   .withColumnRenamed('id', 'station_id')   .withColumn('country_code', F.substring('station_id', 0, 2))\\n\\ndf_countries = spark.read.parquet('gs://ghcnd_raw/ghcnd-countries.parquet')   .withColumnRenamed('name', 'country_name')\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df_stations = spark.read.parquet('gs://ghcnd_raw/ghcnd-stations.parquet') \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.parquet('gs://ghcnd_raw/ghcnd-countries.parquet') \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c589e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing year 2022...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o206.save.\n: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS.<init>()\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryUtil.getUniqueGcsPath(SparkBigQueryUtil.java:112)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryUtil.createGcsPath(SparkBigQueryUtil.java:95)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:44)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:112)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoSuchMethodException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS.<init>()\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2553)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:131)\n\t... 52 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15371/1108152240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprocess_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_countries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprocess_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_stations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_countries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15371/299924469.py\u001b[0m in \u001b[0;36mprocess_year\u001b[0;34m(year, mode, df_stations, df_countries)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;31m# For some reason, partition by date does not work after F.year(\"date\"). This has to be fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;31m# Also, partition is needed for clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mdf_yearly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o206.save.\n: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS.<init>()\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryUtil.getUniqueGcsPath(SparkBigQueryUtil.java:112)\n\tat com.google.cloud.spark.bigquery.SparkBigQueryUtil.createGcsPath(SparkBigQueryUtil.java:95)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.<init>(BigQueryWriteHelper.scala:44)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:42)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:112)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NoSuchMethodException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS.<init>()\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getDeclaredConstructor(Class.java:2553)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:131)\n\t... 52 more\n"
     ]
    }
   ],
   "source": [
    "for year in range(start_year, end_year+1):\n",
    "  if year == start_year:\n",
    "    process_year(year, 'overwrite', df_stations, df_countries)\n",
    "  else:\n",
    "    process_year(year, 'append', df_stations, df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
